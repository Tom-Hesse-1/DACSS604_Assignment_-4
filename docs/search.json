[
  {
    "objectID": "Thomas_Hesse_DACSS_601_Challenge_5/Thomas_Hesse_DACSS_601_Challenge_5.html",
    "href": "Thomas_Hesse_DACSS_601_Challenge_5/Thomas_Hesse_DACSS_601_Challenge_5.html",
    "title": "Thomas_Hesse_DACSS_601_Challenge_5",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\n\n#for plotting time\nlibrary(ggplot2) # if you have not installed this package, please install it.\nlibrary(lubridate)\nlibrary(gganimate)\n\nWarning: package 'gganimate' was built under R version 4.3.2\n\nlibrary(gifski)\n\nWarning: package 'gifski' was built under R version 4.3.2\n\nlibrary(hrbrthemes)\n\nWarning: package 'hrbrthemes' was built under R version 4.3.2\n\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.3.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n#for plotting space\nlibrary(sp)\nlibrary(mdsr)\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(rnaturalearth)\n\nSupport for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`\n\nlibrary(rnaturalearthdata)\n\n\nAttaching package: 'rnaturalearthdata'\n\nThe following object is masked from 'package:rnaturalearth':\n\n    countries110\n\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nPart 1\n\nUFO_Data &lt;- read.csv(\"_data/complete_UFO.csv\")\n\nUFO_Data$datetime &lt;- mdy(UFO_Data$datetime)\n  \n\nhead(UFO_Data)\n\n    datetime                          city state country   shape\n1 1906-11-11                wien (austria)                 other\n2 1910-01-02             kirksville (near)    mo      us    disk\n3 1910-05-28                         solon    me      us unknown\n4 1910-06-01                   wills point    tx      us   cigar\n5 1914-09-15         meeting lake (canada)    ab         unknown\n6 1916-04-05 france (above; from aircraft)                 cigar\n  duration..seconds. duration..hours.min.\n1              10800                  3 h\n2                120              minutes\n3                  0        don&#39t know\n4                120            2 minutes\n5                  0                     \n6                 60         about 1 min.\n                                                                                                                                             comments\n1                                                              The oldest professional photo of a UFO object is from Wien observatory&#44 in 1906&#33\n2                                                                                                 Historical sighting (1903 - 1913) Northern Missouri\n3 entry in my great-grandmother&#39s diary&#44date 28 may 1910&#44refers to watching the comet zig-zaging in the sky&#44 followed by a zig-zag diagra\n4                                                                                                        Cigar shaped object moving from West to East\n5                      Night flying &quot;airplane&quot; with search lights observed on several occasions near Meeting Lake&#44 Alberta in late 1914.\n6                                                                                    ((NUFORC Note:  Possible hoax.  PD))  Saw 3 cigar shaped crafts.\n  date.posted   latitude   longitude  X\n1  12/23/2002  48.208174   16.373819 NA\n2   9/15/2005 40.1947222  -92.583056 NA\n3   12/5/2001 44.9494444  -69.858889 NA\n4   4/16/2005 32.7091667  -96.008056 NA\n5    2/1/2007  55.170828 -118.837956 NA\n6    3/9/2004  46.227638    2.213749 NA\n\ndim(UFO_Data)\n\n[1] 88875    12\n\n\nThe dimension of the complete_UFO data set is 88,875 rows by 12 columns. Each row (unit of observation) represents a distinct UFO sighting. Each column is some variable about that sighting, such as the date and time when the sighting occurred, the city, state and country where the sighting occurred, the shape of the UFO, etc.\n\nUFO_Timeseries_Year &lt;- UFO_Data %&gt;%\n  group_by(year(datetime)) %&gt;%\n  mutate(count = n()) %&gt;%\n  ggplot(aes(x = datetime, y = count)) +\n  geom_line() +\n  labs(title = \"UFO Sightings Spike From 1990s–2010s, Followed by a Sharp Drop\", x = \"Year\", y = \"Number of UFO Sightings\", caption = \"Data From https://www.kaggle.com/datasets/NUFORC/ufo-sightings\") +\n  scale_y_continuous(labels = label_comma()) +\n  scale_x_date(date_breaks = \"5 years\",\n               labels = date_format(\"%Y\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nUFO_Timeseries_Year\n\n\n\n\n\n\n\nUFO_Timeseries_Month &lt;- UFO_Data %&gt;%\n  mutate(month = month(datetime), year = year(datetime)) %&gt;%\n  group_by(month, year) %&gt;%\n  mutate(count = n()) %&gt;%\n  ggplot(aes(x = datetime, y = count)) +\n  geom_line(na.rm = TRUE) +\n  scale_x_date(date_breaks = \"1 month\", \n                 labels=date_format(\"%b %Y\"),\n                 limits = as.Date(c('2010-01-01','2014-01-01'))) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5)) +\n  labs(title = \"Between 01/01/2010–01/01/2014, Reported UFO Sightings Spike Around July\", x = \"Month\", y = \"Number of UFO Sightings\", caption = \"Data From https://www.kaggle.com/datasets/NUFORC/ufo-sightings\")\n\nUFO_Timeseries_Month\n\n\n\n\n\n\n\n\nFrom 1906 (the first year included in the data set) until the early 1940s, the number of reported UFO sightings consistently stay at a zero or near-zero number. Starting in the early 1940s, reported UFO sightings begin to climb slowly year after year, but remain at a relatively small number—only reaching a few hundred sightings per year, at most—for the next 50 years, roughly. In the mid 1990s, the number of reported UFO sightings spikes, with sharp increases as time continues. By the mid 2000s, certain years have thousands of reported UFO sightings, reaching a peak of close to 8,000 sightings in one year, in the early 2010s. In the mid 2010s through the end of the data set, the number of reported UFO sightings dropped sharply again. By month between 2010/01/01 and 2014/01/01, UFO sightings seem to spike in the early-mid summer, around July. Reported UFO sightings seemingly drop off shortly after these spikes, either in August in September, and then experience fluctuations throughout the rest of the year, until the next early-mid summer spike.\nPart 2\n\nhaunted_places &lt;- read_csv(\"_data/haunted_places.csv\")\n\n\nprint(haunted_places)\n\n# A tibble: 10,992 × 10\n   city       country description location state state_abbrev longitude latitude\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ada        United… \"Ada witch… Ada Cem… Mich… MI               -85.5     43.0\n 2 Addison    United… \"A little … North A… Mich… MI               -84.4     42.0\n 3 Adrian     United… \"If you ta… Ghost T… Mich… MI               -84.0     41.9\n 4 Adrian     United… \"In the 19… Siena H… Mich… MI               -84.0     41.9\n 5 Albion     United… \"Kappa Del… Albion … Mich… MI               -84.7     42.2\n 6 Albion     United… \"A mysteri… Riversi… Mich… MI               -84.8     42.2\n 7 Algoma To… United… \"On a wind… Hell's … Mich… MI                NA       NA  \n 8 Algonac    United… \"Morrow Ro… Morrow … Mich… MI               -82.6     42.7\n 9 Allegan    United… \"People re… Elks Lo… Mich… MI               -85.8     42.5\n10 Allegan    United… \"Various g… The Gri… Mich… MI               -85.9     42.5\n# ℹ 10,982 more rows\n# ℹ 2 more variables: city_longitude &lt;dbl&gt;, city_latitude &lt;dbl&gt;\n\ndim(haunted_places)\n\n[1] 10992    10\n\n\nThe haunted_places data set has a dimension of 10,992 rows by 10 columns. The unit of observation is a unique haunted place. Each column is some variable about that haunted place, such as the state it is located in , the coordinates of the place, the coordinates of the city the place is located in, etc.\n\nstates_sf&lt;-read_sf(file.path(\"C:/Users/Colonel/OneDrive/Desktop/Thomas_Hesse_DACSS_601_Challenge_5/Thomas_Hesse_DACSS_601_Challenge_5/state/cb_2018_us_state_500k.shp\"))\n\n\nstates &lt;- ggplot()+\n  geom_sf(data = states_sf)+\n  coord_sf(xlim = c(-180, -65),\n          ylim = c(20, 70))+\n  theme_light()\n\nstates\n\n\n\n\n\n\n\n\n\nhaunted_states &lt;- states +\n  geom_point(data = haunted_places, aes(x = city_longitude, y = city_latitude), size = 0.1, na.rm = TRUE) +\n  labs(title = \"Locations of Haunted Places Cluster Around Population Centers\", x = \"Longitude\", y = \"Latitude\", caption = \"Data From https://www.kaggle.com/datasets/sujaykapadnis/haunted-places\")\n\nhaunted_states\n\n\n\n\n\n\n\n\nBased on the map above, the locations of haunted places seem to cluster around a number of spots within the US. The largest clusters seem to be located in the Northeast, Mid Atlantic, Great Lake, and Piedmont Atlantic regions of the US. Smaller clusters of haunted places are located around the Southern California, Northern California, Cascadia, Florida, Gulf Coast-Texas, and Central-Texas regions, as well as scattered throughout the midwest, great plains, and western regions. Despite a few smaller clusters, much of the space between the midwest and west coast has very few reports of haunted places, relative to the rest of the US. Outside of the contiguous US, Alaska seems to have very few reports of haunted places, and Hawaii seems to have a small cluster located on its islands."
  },
  {
    "objectID": "Thomas_Hesse_DACSS_601_Challenge_5/Thomas_Hesse_DACSS_601_Challenge_5.html#quarto",
    "href": "Thomas_Hesse_DACSS_601_Challenge_5/Thomas_Hesse_DACSS_601_Challenge_5.html#quarto",
    "title": "Thomas_Hesse_DACSS_601_Challenge_5",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\n\n#for plotting time\nlibrary(ggplot2) # if you have not installed this package, please install it.\nlibrary(lubridate)\nlibrary(gganimate)\n\nWarning: package 'gganimate' was built under R version 4.3.2\n\nlibrary(gifski)\n\nWarning: package 'gifski' was built under R version 4.3.2\n\nlibrary(hrbrthemes)\n\nWarning: package 'hrbrthemes' was built under R version 4.3.2\n\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.3.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n#for plotting space\nlibrary(sp)\nlibrary(mdsr)\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(rnaturalearth)\n\nSupport for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`\n\nlibrary(rnaturalearthdata)\n\n\nAttaching package: 'rnaturalearthdata'\n\nThe following object is masked from 'package:rnaturalearth':\n\n    countries110\n\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nPart 1\n\nUFO_Data &lt;- read.csv(\"_data/complete_UFO.csv\")\n\nUFO_Data$datetime &lt;- mdy(UFO_Data$datetime)\n  \n\nhead(UFO_Data)\n\n    datetime                          city state country   shape\n1 1906-11-11                wien (austria)                 other\n2 1910-01-02             kirksville (near)    mo      us    disk\n3 1910-05-28                         solon    me      us unknown\n4 1910-06-01                   wills point    tx      us   cigar\n5 1914-09-15         meeting lake (canada)    ab         unknown\n6 1916-04-05 france (above; from aircraft)                 cigar\n  duration..seconds. duration..hours.min.\n1              10800                  3 h\n2                120              minutes\n3                  0        don&#39t know\n4                120            2 minutes\n5                  0                     \n6                 60         about 1 min.\n                                                                                                                                             comments\n1                                                              The oldest professional photo of a UFO object is from Wien observatory&#44 in 1906&#33\n2                                                                                                 Historical sighting (1903 - 1913) Northern Missouri\n3 entry in my great-grandmother&#39s diary&#44date 28 may 1910&#44refers to watching the comet zig-zaging in the sky&#44 followed by a zig-zag diagra\n4                                                                                                        Cigar shaped object moving from West to East\n5                      Night flying &quot;airplane&quot; with search lights observed on several occasions near Meeting Lake&#44 Alberta in late 1914.\n6                                                                                    ((NUFORC Note:  Possible hoax.  PD))  Saw 3 cigar shaped crafts.\n  date.posted   latitude   longitude  X\n1  12/23/2002  48.208174   16.373819 NA\n2   9/15/2005 40.1947222  -92.583056 NA\n3   12/5/2001 44.9494444  -69.858889 NA\n4   4/16/2005 32.7091667  -96.008056 NA\n5    2/1/2007  55.170828 -118.837956 NA\n6    3/9/2004  46.227638    2.213749 NA\n\ndim(UFO_Data)\n\n[1] 88875    12\n\n\nThe dimension of the complete_UFO data set is 88,875 rows by 12 columns. Each row (unit of observation) represents a distinct UFO sighting. Each column is some variable about that sighting, such as the date and time when the sighting occurred, the city, state and country where the sighting occurred, the shape of the UFO, etc.\n\nUFO_Timeseries_Year &lt;- UFO_Data %&gt;%\n  group_by(year(datetime)) %&gt;%\n  mutate(count = n()) %&gt;%\n  ggplot(aes(x = datetime, y = count)) +\n  geom_line() +\n  labs(title = \"UFO Sightings Spike From 1990s–2010s, Followed by a Sharp Drop\", x = \"Year\", y = \"Number of UFO Sightings\", caption = \"Data From https://www.kaggle.com/datasets/NUFORC/ufo-sightings\") +\n  scale_y_continuous(labels = label_comma()) +\n  scale_x_date(date_breaks = \"5 years\",\n               labels = date_format(\"%Y\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nUFO_Timeseries_Year\n\n\n\n\n\n\n\nUFO_Timeseries_Month &lt;- UFO_Data %&gt;%\n  mutate(month = month(datetime), year = year(datetime)) %&gt;%\n  group_by(month, year) %&gt;%\n  mutate(count = n()) %&gt;%\n  ggplot(aes(x = datetime, y = count)) +\n  geom_line(na.rm = TRUE) +\n  scale_x_date(date_breaks = \"1 month\", \n                 labels=date_format(\"%b %Y\"),\n                 limits = as.Date(c('2010-01-01','2014-01-01'))) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5)) +\n  labs(title = \"Between 01/01/2010–01/01/2014, Reported UFO Sightings Spike Around July\", x = \"Month\", y = \"Number of UFO Sightings\", caption = \"Data From https://www.kaggle.com/datasets/NUFORC/ufo-sightings\")\n\nUFO_Timeseries_Month\n\n\n\n\n\n\n\n\nFrom 1906 (the first year included in the data set) until the early 1940s, the number of reported UFO sightings consistently stay at a zero or near-zero number. Starting in the early 1940s, reported UFO sightings begin to climb slowly year after year, but remain at a relatively small number—only reaching a few hundred sightings per year, at most—for the next 50 years, roughly. In the mid 1990s, the number of reported UFO sightings spikes, with sharp increases as time continues. By the mid 2000s, certain years have thousands of reported UFO sightings, reaching a peak of close to 8,000 sightings in one year, in the early 2010s. In the mid 2010s through the end of the data set, the number of reported UFO sightings dropped sharply again. By month between 2010/01/01 and 2014/01/01, UFO sightings seem to spike in the early-mid summer, around July. Reported UFO sightings seemingly drop off shortly after these spikes, either in August in September, and then experience fluctuations throughout the rest of the year, until the next early-mid summer spike.\nPart 2\n\nhaunted_places &lt;- read_csv(\"_data/haunted_places.csv\")\n\n\nprint(haunted_places)\n\n# A tibble: 10,992 × 10\n   city       country description location state state_abbrev longitude latitude\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n 1 Ada        United… \"Ada witch… Ada Cem… Mich… MI               -85.5     43.0\n 2 Addison    United… \"A little … North A… Mich… MI               -84.4     42.0\n 3 Adrian     United… \"If you ta… Ghost T… Mich… MI               -84.0     41.9\n 4 Adrian     United… \"In the 19… Siena H… Mich… MI               -84.0     41.9\n 5 Albion     United… \"Kappa Del… Albion … Mich… MI               -84.7     42.2\n 6 Albion     United… \"A mysteri… Riversi… Mich… MI               -84.8     42.2\n 7 Algoma To… United… \"On a wind… Hell's … Mich… MI                NA       NA  \n 8 Algonac    United… \"Morrow Ro… Morrow … Mich… MI               -82.6     42.7\n 9 Allegan    United… \"People re… Elks Lo… Mich… MI               -85.8     42.5\n10 Allegan    United… \"Various g… The Gri… Mich… MI               -85.9     42.5\n# ℹ 10,982 more rows\n# ℹ 2 more variables: city_longitude &lt;dbl&gt;, city_latitude &lt;dbl&gt;\n\ndim(haunted_places)\n\n[1] 10992    10\n\n\nThe haunted_places data set has a dimension of 10,992 rows by 10 columns. The unit of observation is a unique haunted place. Each column is some variable about that haunted place, such as the state it is located in , the coordinates of the place, the coordinates of the city the place is located in, etc.\n\nstates_sf&lt;-read_sf(file.path(\"C:/Users/Colonel/OneDrive/Desktop/Thomas_Hesse_DACSS_601_Challenge_5/Thomas_Hesse_DACSS_601_Challenge_5/state/cb_2018_us_state_500k.shp\"))\n\n\nstates &lt;- ggplot()+\n  geom_sf(data = states_sf)+\n  coord_sf(xlim = c(-180, -65),\n          ylim = c(20, 70))+\n  theme_light()\n\nstates\n\n\n\n\n\n\n\n\n\nhaunted_states &lt;- states +\n  geom_point(data = haunted_places, aes(x = city_longitude, y = city_latitude), size = 0.1, na.rm = TRUE) +\n  labs(title = \"Locations of Haunted Places Cluster Around Population Centers\", x = \"Longitude\", y = \"Latitude\", caption = \"Data From https://www.kaggle.com/datasets/sujaykapadnis/haunted-places\")\n\nhaunted_states\n\n\n\n\n\n\n\n\nBased on the map above, the locations of haunted places seem to cluster around a number of spots within the US. The largest clusters seem to be located in the Northeast, Mid Atlantic, Great Lake, and Piedmont Atlantic regions of the US. Smaller clusters of haunted places are located around the Southern California, Northern California, Cascadia, Florida, Gulf Coast-Texas, and Central-Texas regions, as well as scattered throughout the midwest, great plains, and western regions. Despite a few smaller clusters, much of the space between the midwest and west coast has very few reports of haunted places, relative to the rest of the US. Outside of the contiguous US, Alaska seems to have very few reports of haunted places, and Hawaii seems to have a small cluster located on its islands."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This Project was an exercise in R, analyzing and visualizing geographic and time-based data. The objective was to practice writing code that analyzes and presents these types of data. This exercised examined the temporal and geographic distributions of UFO sightings. Project 1"
  },
  {
    "objectID": "projects.html#project-1-practice-visualizing-geographic-and-time-based-data-in-r",
    "href": "projects.html#project-1-practice-visualizing-geographic-and-time-based-data-in-r",
    "title": "Projects",
    "section": "",
    "text": "This Project was an exercise in R, analyzing and visualizing geographic and time-based data. The objective was to practice writing code that analyzes and presents these types of data. This exercised examined the temporal and geographic distributions of UFO sightings. Project 1"
  },
  {
    "objectID": "projects.html#project-2-dashboard",
    "href": "projects.html#project-2-dashboard",
    "title": "Projects",
    "section": "Project 2: Dashboard",
    "text": "Project 2: Dashboard\nThis dashboard shows the distribution of US households by social class. Project 2"
  },
  {
    "objectID": "Project1/starbucks.html",
    "href": "Project1/starbucks.html",
    "title": "Interactive Map",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "Project1/starbucks.html#setup",
    "href": "Project1/starbucks.html#setup",
    "title": "Interactive Map",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nlibrary(stringr) # if you have not installed this package, please install it.\nlibrary(ggplot2) # if you have not installed this package, please install it.\nlibrary(sf) \nlibrary(mapview)\n#Loading colorblind-friendly color map package: viridisLite\nlibrary(viridis)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "Project1/starbucks.html#part-1.-starbucks-map",
    "href": "Project1/starbucks.html#part-1.-starbucks-map",
    "title": "Interactive Map",
    "section": "Part 1. Starbucks Map",
    "text": "Part 1. Starbucks Map\n\nLoading the example data of starbuck locations\n\n\n#read the data: \nstarbucks &lt;- read_csv(\"https://raw.githubusercontent.com/libjohn/mapping-with-R/master/data/All_Starbucks_Locations_in_the_US_-_Map.csv\", show_col_types = FALSE)\nhead(starbucks)\n\n\n  \n\n\n##Create a column of \"gross profit\" and assign random numbers to it:\nstarbucks &lt;- starbucks |&gt;\n  mutate(gross_profit = sample(1000:10000, size = n(), replace = TRUE))\n\nhead(starbucks)\n\n\n  \n\n\n\nsubset locations data to MA\n\nstarbucksMA &lt;- starbucks  |&gt;\n  filter(State %in% c(\"MA\"))\n\nwrite.csv(starbucksMA, \"starbucksMA.csv\", row.names = FALSE)\n\nConvert the dataset into a spatial object (sf)\n\nstarbucks_sf &lt;- starbucksMA |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nlet’s compare the difference between using size and brightness\nSize:\n\nmapview(starbucks_sf, col.regions = \"red\", cex = \"gross_profit\", legend = TRUE, map.types = \"OpenStreetMap\")\n\n\n\n\n#col.region: Sets all bubbles to a fixed color\n#cex:sets the bubble size to be proportional to a specific column\n\nBrightness:\n\nmapview(starbucks_sf, zcol = \"gross_profit\", cex = 3, legend = TRUE, map.types = \"OpenStreetMap\")\n\n\n\n\n#zcol = specifies that the colors (brightness or color/hue depends on if the variable is continous or not) of the bubbles should vary by the values of a particular column"
  },
  {
    "objectID": "Project1/starbucks.html#part-2.-animated-graph-mtcars",
    "href": "Project1/starbucks.html#part-2.-animated-graph-mtcars",
    "title": "Interactive Map",
    "section": "Part 2. Animated Graph (mtcars)",
    "text": "Part 2. Animated Graph (mtcars)\n\nlibrary(gganimate)\nlibrary(gifski)\n\n# Example animation\np &lt;- ggplot(mtcars, aes(mpg, wt)) +\n  geom_point() +\n  transition_states(gear, transition_length = 2, state_length = 1) +\n  ggtitle('Gear: {closest_state}')\n\n# Use gifski_renderer for GIF animations\nanimate(p, renderer = gifski_renderer())\n\n\n\n\n\n\n\n# You can try different setup, such as duration, fps, size of the graph):\n#animate(p, duration = 25, fps = 10, width = 1000, height = 1000, renderer = #gifski_renderer())"
  },
  {
    "objectID": "Input Sidebar.html",
    "href": "Input Sidebar.html",
    "title": "Sidebar",
    "section": "",
    "text": "# shiny inputs defined here"
  },
  {
    "objectID": "Input Sidebar.html#inputs",
    "href": "Input Sidebar.html#inputs",
    "title": "Sidebar",
    "section": "",
    "text": "# shiny inputs defined here"
  },
  {
    "objectID": "Input Sidebar.html#column",
    "href": "Input Sidebar.html#column",
    "title": "Sidebar",
    "section": "Column",
    "text": "Column\n\nChart 1\n\n\nChart 2"
  },
  {
    "objectID": "flexdashboard1.html",
    "href": "flexdashboard1.html",
    "title": "Assignment4FlexDashboard",
    "section": "",
    "text": "GSS2022 &lt;- read_dta(\"data/GSS2022.dta\")\nGSS2022 &lt;- GSS2022 %&gt;%\n  mutate(class = as.integer(class))\n\n\nclass_hist &lt;- GSS2022 %&gt;%\n  drop_na(class) %&gt;%\n  mutate(class = recode(class, \"1\" = \"Lower Class\", \"2\" = \"Working Class\", \"3\" = \"Middle Class\", \"4\" = \"Upper Class\")) %&gt;%\n  ggplot(aes(x=class)) +\n  geom_bar(color = \"black\") +\n  scale_x_discrete(limits = c(\"Lower Class\", \"Working Class\", \"Middle Class\", \"Upper Class\")) +\n  scale_y_continuous(label=comma) +\n  labs(title = \"Most Americans Identify as Working or Middle Class\",\n       x = \"Social Class\",\n       y = \"Count\",\n       caption = \"Data from 2022 General Social Survey (GSS), NORC\") +\n  theme(plot.title = element_text(size = 10))"
  },
  {
    "objectID": "flexdashboard1.html#column",
    "href": "flexdashboard1.html#column",
    "title": "Assignment4FlexDashboard",
    "section": "Column",
    "text": "Column\n\nChart A\n\nclass_hist"
  },
  {
    "objectID": "flexdashboard1.html#column-1",
    "href": "flexdashboard1.html#column-1",
    "title": "Assignment4FlexDashboard",
    "section": "Column",
    "text": "Column\n\nChart B\n\n\nChart C"
  },
  {
    "objectID": "flexboard.html",
    "href": "flexboard.html",
    "title": "Storyboard Commentary",
    "section": "",
    "text": "Frame 1\n\nSome commentary about Frame 1.\n\n\nFrame 2\n\nSome commentary about Frame 2."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Tom Hesse. I am a student in the University of Masters of Science in Data Analytics & Computational Social Science program at the University of Massachusetts Amherst. In undergraduate, I studied Criminal Justice and Psychology at Westfield State University. I am broadly interested in the social sciences, but my specific research interests include demographics, state and national politics, human geography, crime & human behavior, and more. My most recent projects have focused on the rural-urban divide in America, and how immigration is connected to economic growth.\n\n\n\nOpen Resume"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "About Me",
    "section": "",
    "text": "My name is Tom Hesse. I am a student in the University of Masters of Science in Data Analytics & Computational Social Science program at the University of Massachusetts Amherst. In undergraduate, I studied Criminal Justice and Psychology at Westfield State University. I am broadly interested in the social sciences, but my specific research interests include demographics, state and national politics, human geography, crime & human behavior, and more. My most recent projects have focused on the rural-urban divide in America, and how immigration is connected to economic growth."
  },
  {
    "objectID": "index.html#my-resume",
    "href": "index.html#my-resume",
    "title": "About Me",
    "section": "",
    "text": "Open Resume"
  },
  {
    "objectID": "multiple_pages.html",
    "href": "multiple_pages.html",
    "title": "Multiple Pages",
    "section": "",
    "text": "#read the data: \nstarbucks &lt;- read_csv(\"https://raw.githubusercontent.com/libjohn/mapping-with-R/master/data/All_Starbucks_Locations_in_the_US_-_Map.csv\", show_col_types = FALSE)\n#head(starbucks)\n\n##Create a column of \"gross profit\" and assign random numbers to it:\nstarbucks &lt;- starbucks |&gt;\n  mutate(gross_profit = sample(1000:10000, size = n(), replace = TRUE))\n#head(starbucks)\n\nstarbucksMA &lt;- starbucks  |&gt;\n  filter(State %in% c(\"MA\"))\n\nstarbucks_sf &lt;- starbucksMA |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nmapview(starbucks_sf, col.regions = \"red\", cex = \"gross_profit\", legend = TRUE, map.types = \"OpenStreetMap\")\n\n\n\n\n#col.region: Sets all bubbles to a fixed color\n#cex:sets the bubble size to be proportional to a specific column\n\n\n\n\n\n\n\nSource: Starbucks Location in the US\nIn this visualization, we subset the data by keeping only starbucks in Massachusetts.\n\n\n\nStarbucks are more condensed in urban area, such as the greater Boston and Springfield."
  },
  {
    "objectID": "multiple_pages.html#column",
    "href": "multiple_pages.html#column",
    "title": "Multiple Pages",
    "section": "",
    "text": "#read the data: \nstarbucks &lt;- read_csv(\"https://raw.githubusercontent.com/libjohn/mapping-with-R/master/data/All_Starbucks_Locations_in_the_US_-_Map.csv\", show_col_types = FALSE)\n#head(starbucks)\n\n##Create a column of \"gross profit\" and assign random numbers to it:\nstarbucks &lt;- starbucks |&gt;\n  mutate(gross_profit = sample(1000:10000, size = n(), replace = TRUE))\n#head(starbucks)\n\nstarbucksMA &lt;- starbucks  |&gt;\n  filter(State %in% c(\"MA\"))\n\nstarbucks_sf &lt;- starbucksMA |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n\nmapview(starbucks_sf, col.regions = \"red\", cex = \"gross_profit\", legend = TRUE, map.types = \"OpenStreetMap\")\n\n\n\n\n#col.region: Sets all bubbles to a fixed color\n#cex:sets the bubble size to be proportional to a specific column"
  },
  {
    "objectID": "multiple_pages.html#column-1",
    "href": "multiple_pages.html#column-1",
    "title": "Multiple Pages",
    "section": "",
    "text": "Source: Starbucks Location in the US\nIn this visualization, we subset the data by keeping only starbucks in Massachusetts.\n\n\n\nStarbucks are more condensed in urban area, such as the greater Boston and Springfield."
  },
  {
    "objectID": "multiple_pages.html#row",
    "href": "multiple_pages.html#row",
    "title": "Multiple Pages",
    "section": "Row",
    "text": "Row\n\nChart 1"
  },
  {
    "objectID": "multiple_pages.html#row-1",
    "href": "multiple_pages.html#row-1",
    "title": "Multiple Pages",
    "section": "Row",
    "text": "Row\n\nChart 2\n\n\nChart 3"
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html",
    "href": "Project2/challenge_4_Fall23_solutions.html",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "",
    "text": "Make sure you change the author’s name in the above YAML header."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#setup",
    "href": "Project2/challenge_4_Fall23_solutions.html#setup",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Setup",
    "text": "Setup\nIf you have not installed the following packages, please install them before loading them.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)\nlibrary(stringr) # if you have not installed this package, please install it.\nlibrary(ggplot2) # if you have not installed this package, please install it.\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#challenge-overview",
    "href": "Project2/challenge_4_Fall23_solutions.html#challenge-overview",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nIn this challenge, we will practice with the data we worked on in the previous challenges and the data you choose to do some simple data visualizations using the ggplot2 package.\nThere will be coding components and writing components. Please read the instructions for each part and complete your challenges."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#datasets",
    "href": "Project2/challenge_4_Fall23_solutions.html#datasets",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Datasets",
    "text": "Datasets\n\nPart 1 the ESS_Polity Data (created in Challenge#3) ⭐⭐\nPart 2: the Australia Data⭐⭐\nPart 3: see Part 3. Practice plotting with a dataset of your choice (25% of the total grade). For online platforms of free data, see Appendix: sources for data to be used in Part 3.\n\nFind the _data folder, then read the datasets using the correct R command."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#part-1.-univariate-and-multivariate-graphs-45-of-the-total-grade",
    "href": "Project2/challenge_4_Fall23_solutions.html#part-1.-univariate-and-multivariate-graphs-45-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 1. Univariate and Multivariate Graphs (45% of the total grade)",
    "text": "Part 1. Univariate and Multivariate Graphs (45% of the total grade)\nWe have been working with these two data in the previous three challenges. Suppose we have a research project that studies European citizens’ social behaviors and public opinions, and we are interested in how the countries that respondents live in influence their behavior and opinion. In this challenge, let’s work with the combined dataset ESS_Polity and create some visualizations.\n\nRead the combined data you created last time. (2.5%)\n\n\n#read the data: \nESS_Polity &lt;- read_csv(\"ESS_Polity.csv\")\n\nSuppose we are interested in the central tendencies and distributions of the following variables. At the individual level: age, male, edu, income_10, and vote. At the country level: democ.\n(1) Recode the “vote” column: if the value is 1, recode it as 1; if the value is 2, recode it as 0; if the value is 3, recode it as NA. Make sure to include a sanity check for the recoded data. (2.5%)\n\n#recoding the vote column: 1.5%\nESS_Polity&lt;-ESS_Polity|&gt;\n  mutate(vote = case_when(\n    vote == 1 ~ 1,\n    vote == 2 ~ 0,\n    vote == 3 ~ NA,\n    TRUE ~ vote))\n\n#Sanity check for if vote is correctly coded: 1%\nunique(ESS_Polity$vote)\n\n[1] NA  0  1\n\n\n(2) For each of the five variables (age, edu, income_10, vote, and democ), please choose an appropriate type of univariate graph to plot the central tendencies and distribution of the variables. Explain why you choose this type of graph to present a particular variable (for example: “For example, I use a histogram to plot age because it is a continuous numeric variable”). (25%)\n(Note: You should use at least two types of univariate graphs covered in the lecture.)\nAnswer: First, I do summary statistics for the five variables to check their range, number of values, and measurement. This step would help me to determine which type of graph I should choose. This is optional, but strongly recommended when you are working on any data projects.\n\n##I am using a user-defined function created in Challenge#3, but you can use other descriptive statistics functions along with the baseR (such as summary()) or other packages.\n\nsum_stat &lt;- function(x){\n  stat &lt;- tibble(\n    range=range(x, na.rm = T),\n    mean=mean(x, na.rm = T),\n    sd=sd(x,na.rm=T),\n    na = sum(is.na(x)),\n    unique = length(unique(x)),\n    class = typeof(x)\n  )\n  return(stat)\n}\n\nsum_stat(ESS_Polity$age) #has 88 unique values and it is numeric (double): continous variable. Appropriate univariate graph type: boxplot, violin chart, density, histogram\n\n\n  \n\n\nsum_stat(ESS_Polity$edu) #has 4 unique values and it is numeric (double): continous or ordinal categorial variable (depends on if it is originally coded as the level of education or the years of education). Appropriate univariate graph type: histogram or barplot \n\n\n  \n\n\nsum_stat(ESS_Polity$income_10) #has 10 unique values and it is numeric (double):orordinal categorial variable (10 income groups, so can be treated as a contionus variable). Appropriate univariate graph type: boxplot, violin chart, histogram\n\n\n  \n\n\nsum_stat(ESS_Polity$vote)#has 2 unique values and it is numeric (double): binary variable (only 1 and 0). Appropriate univariate graph type: scatterplot or barplot\n\n\n  \n\n\nsum_stat(ESS_Polity$democ)#has 6 unique values and it is numeric (double): ordinal categorial variable (originally a contionus variable, but since there are less than 10 values, we can treat it as an ordinal categorical variable when plotting): bar plot \n\n\n  \n\n\n\nIf you only plotted the figure without explaining why you chose the specific type of figure, and if you chose an inappropriate type of figure for a variable: - 4%.\nIf you give explanations on why you chose the specific type of figure, and it is not an inappropriate type of figure for that variable: - 2%.\n\nAge:\n\n\n#For age, I choose a box plot. Noted that when we plot a boxplot, NA are automatically removed.\n\nage_boxplot&lt;-ggplot(ESS_Polity) +\n  geom_boxplot(aes(x = age), fill=\"slateblue\", alpha=0.2)+\n    labs(title = \"Distribution of Respondents' Age\",\n       x = \"Age\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n  \n\nage_boxplot\n\n\n\n\n\n\n\n\nOr I can do a histogram. Note that it is important to select an appropriate bin size. In this example, a bin size between 4 and 8 should show a similar distirbution pattern. If you use a different bin size: - 1%\n\n#Noted that when we plot a histogram, NA are automatically removed.\nage_hist&lt;-ggplot(ESS_Polity) +\n  geom_histogram(binwidth=4, aes(x = age), fill=\"slateblue\", alpha=0.2)+\n    labs(title = \"Distribution of Respondents' Age\",\n       x = \"Age\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n  \n\nage_hist\n\n\n\n\n\n\n\n\n\nEducation:\n\n\n#For edu, I can also choose a bar plot:\nedu_bar &lt;- ggplot(ESS_Polity, aes(x=edu)) + \n  geom_bar()+\n  labs(title = \"Distribution of Respondents' Levels of Education\",\n       x = \"Levels of Education\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\nedu_bar\n\n\n\n\n\n\n\n\nNoted that when we plot a bar plot, NAs are not automatically removed (the warning message showing 150). NA has no numerical meaning (and should not be ranked as the highest in visualization). Generally for plots showing data distribution, we should remove it from the graphs (if not removing NA in a barplot orf showing data distribution, we can remove it from the graphs (if not removing NA in a barplot or : -1%).\n\nedu_bar &lt;- ggplot(data = subset(ESS_Polity, !is.na(edu)), aes(x = as.factor(edu))) + \n  geom_bar()+\n  labs(title = \"Distribution of Respondents' Levels of Education\",\n       x = \"Levels of Education (na removed)\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n  \nedu_bar\n\n\n\n\n\n\n\n\nAs we can see, the shape of the bars is similar to the graph without removing NA, since there are few NAs in education.\n\nIncome:\n\n\n#For income groups, I can also choose a bar plot:\n\nincome_bar &lt;- ggplot(ESS_Polity, aes(x=as.factor(income_10))) + #I use as.factor to force R to recognize income_10 as an ordinal category, so that the x-axis tick mark labels can automatically represent all categories.\n  geom_bar(fill=\"#69b3a2\", color=\"#e9ecef\", alpha=0.8)+\n  labs(title = \"Distribution of Respondents' Income Levels\",\n       x = \"Income Levels (10 ordinal groups)\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n\nincome_bar\n\n\n\n\n\n\n\n\n\nIn this graph, we can see that a huge proportion of the income data is NAs. They actually affect the presentation by making the shape of the distribution not so obvious. Let’s remove them.\n\n\nincome_bar &lt;- ggplot(subset(ESS_Polity, !is.na(income_10)), aes(x=as.factor(income_10))) + \n  geom_bar(fill=\"#69b3a2\", color=\"#e9ecef\", alpha=0.8)+\n  labs(title = \"Distribution of Respondents' Income Levels\",\n       x = \"Income Levels (10 ordinal groups, na removed)\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n\nincome_bar\n\n\n\n\n\n\n\n\n\nVote:\n\n\n#For vote, I choose a bar plot.\n\nvote_bar &lt;- ggplot(subset(ESS_Polity, !is.na(vote)), aes(x=as.factor(vote))) + \n  geom_bar(fill=\"blue\", color=\"grey\", alpha=0.8)+\n  labs(title = \"Distribution of Respondents' Voter Turnout\",\n       x = \"Vote Choice (binary, na removed)\",\n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")+\n  scale_x_discrete(labels=c(\"0\" = \"Didn't Vote\", \"1\" = \"Voted\"))\n\nvote_bar\n\n\n\n\n\n\n\n\n\nDemocracy:\n\n\n#For democ, I choose a bar plot.\n\ndemocracy_bar &lt;- ESS_Polity|&gt;\n  subset(!is.na(democ))|&gt;\n  ggplot(aes(x=as.factor(democ))) + \n  geom_bar(fill=\"red\", color=\"grey\", alpha=0.8)+\n  labs(title = \"Distribution of the Democracy Score of Countries by Respondents\",\n       x = \"Democracy Score (na removed)\",\n       y = \"Count of Respondents\", \n       caption = \"source: combined dataset of ESS (round 2010) and Polity V\")\n\ndemocracy_bar \n\n\n\n\n\n\n\n\n\nWe want to test two hypotheses on the relationships of two pairs of variables. Please use the appropriate type of graphs we learned to visualize these two pairs of variables. Briefly describe the graph you plot, and answer: Does the graph we create from the data support the hypothesis?\n\n\n\n(1) Hypothesis#1: The more years of education (edu) a person completed, the higher income (income_10) they earn. (7.5%)\nAnswer: edu is a variable with only four values. We can treat it as an ordinal categorical variable. Income has 10 values, and we can treat it as either a continuous or an ordinal categorical variable. In this case, we have several options: a stacked bar or a groupped bar.\n\n#don't forget to remove NAs (if not, -2%)\nedu_income&lt;-ESS_Polity|&gt;\n  subset(!is.na(income_10))|&gt; #remove na in income\n  subset(!is.na(edu))|&gt; #remove na in edu\n  ggplot(aes(x = as.factor(income_10), fill = as.factor(edu))) + \n    geom_bar()\n\nedu_income\n\n\n\n\n\n\n\n\nWe can clearly see as in the higher income group, the proportion of highest education level (4) increases. This is more obvious if we do a stacked percentage bar plot.\n\nedu_income&lt;-ESS_Polity|&gt;\n  subset(!is.na(income_10))|&gt; #remove na in income\n  subset(!is.na(edu))|&gt; #remove na in edu\n  ggplot(aes(x = as.factor(income_10), fill = as.factor(edu))) + \n    geom_bar(position=\"fill\")\n\nedu_income\n\n\n\n\n\n\n\n\nWe can also do a boxplot of income grouped (since income has 10 values, we can treat it as a continous variable) by education level:\n\nedu_income&lt;-ESS_Polity|&gt;\n  subset(!is.na(income_10))|&gt; #remove na in income\n  subset(!is.na(edu))|&gt; \n  ggplot(aes (x = as.factor(edu), y = income_10)) +\n  geom_boxplot(fill=\"slateblue\", alpha=0.2) \n\nedu_income\n\n\n\n\n\n\n\n\nHowever, scatter plots will look very strange and it is hard to estimate the pattern of variables with less than 10 values.\n\nedu_income&lt;-ESS_Polity|&gt;\n  subset(!is.na(income_10))|&gt; #remove na in income\n  subset(!is.na(edu))|&gt; \n  ggplot(aes(x = as.factor(income_10), y = as.factor(edu))) + \n  geom_point()+\n  geom_smooth()\n\nedu_income\n\n\n\n\n\n\n\n\nSo in conclusion, this hypothesis is supportive.\n(2) Hypothesis#2: There is a gender disparity (male) in voting behavior (vote). (Either men are more likely to vote, or women are more likely to vote). (7.5%)\nAnswer: both gender and vote are binary variables. So our option is either bar plots or .\n\nmale_vote&lt;-ESS_Polity|&gt;\n  subset(!is.na(male))|&gt; #remove na in income\n  subset(!is.na(vote))|&gt; \n  ggplot(aes(x = as.factor(male), fill = as.factor(vote))) + \n    geom_bar(position=\"fill\")\n\nmale_vote\n\n\n\n\n\n\n\n\n\nHmm, the average turnouts of male voters and female voters are very similar. It does seem that gender determines people’s voting decisions. In fact, if you use group_by and summarise(), you will find that one is 0.767 (female), and another is 0.759 (male). In conclusion, the second hypothesis does seem to be valid.\n\nESS_Polity|&gt;\n  group_by(male)|&gt;\n  subset(!is.na(male))|&gt; #remove na in income\n  subset(!is.na(vote))|&gt;\n  summarise(mean(vote))"
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#part-2.-comparing-between-partial-and-whole-and-among-groups-30-of-the-total-grade",
    "href": "Project2/challenge_4_Fall23_solutions.html#part-2.-comparing-between-partial-and-whole-and-among-groups-30-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 2. Comparing between Partial and Whole, and among Groups (30% of the total grade)",
    "text": "Part 2. Comparing between Partial and Whole, and among Groups (30% of the total grade)\nIn this part, we will use the clean version of the Australian public opinion poll on Same-Sex Marriage to generate graphs and plots. You may need to do the data transformation or mutation needed to help graphing.\n\nRead in data. (2.5%)\n\naustralian_data &lt;- read_csv(\"australian_data.csv\")\n\nUse a barplot to graph the Australian data based on their responses: yes, no, illegible, and no response. The y-axis should be the count of responses, and each response should be represented by one individual bar (so there should be four bars). (7.5%)\n(you can use either geom_bar() or geom_col())\nAnswer: First we need to reshape the data to convert it:\n\naus_long &lt;- australian_data |&gt;\n  pivot_longer(\ncols = Yes:`No Response`,\nnames_to = \"Response\",\nvalues_to = \"Count\"\n  )\nhead(aus_long)\n\n\n  \n\n\n\nAfter reshaping the data, we can plot it now:\n\n#Plot the barchart for repsonses:\n\nresponse_bar&lt;-ggplot(aus_long, aes(x=Response, y=Count))+\n  geom_bar(stat=\"identity\",fill=\"purple\")\n\nresponse_bar\n\n\n\n\n\n\n\n\nWe can customize the bar plot by reordering the bars, presenting the raw numbers of each response, and editing title and labels. We even change the y-axis tick marks from count to percentage (not required in the question).\n\n\nresponse_bar&lt;-aus_long|&gt;\n  mutate(Response = as_factor(Response), #we need to first force R to recognize \"Response\" as factor\n         Response = fct_relevel(Response, \"Yes\", \"No\", \"Illegible\"))|&gt; #then we can use fct_relvel to specify the order of the bars.\n  group_by(Response)|&gt;\n  summarise(Count = sum(Count))|&gt; # try without specifying group_by and ungroup, what do we got?\n  ungroup()|&gt; \n  mutate(perc = Count/sum(Count))|&gt;\n  ggplot(aes(y=perc, x=Response))+\n  geom_col()+\n  labs(title = \"The National Distribution of Resesponse\")+\n  scale_y_continuous(name= \"Percent of Citizens\", \n                     label = scales::percent) +\n  geom_text(aes(label = Count), size=3, vjust=-.5)\n  \nresponse_bar\n\n\n\n\n\n\n\n\n\nThe previous graph only shows the difference in amount. Let’s create a stacked-to-100% barplot to show the proportion of each of the four responses (by % of the total response). (7.5%)\n(you can use either geom_bar() or geom_col())\n\n#We will use the original data to plot this stacked-to-100% bar.\n\nresponse_stack&lt;-ggplot(aus_long, aes(fill = Response, x = '', y = Count)) + \n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(title = \"The National Distribution of Resesponse\", x = NULL, y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 100))\n\nresponse_stack\n\n\n\n\n\n\n\n\nLet’s see if there’s a relationship between Division and Response - that is, are certain divisions more likely to respond one way compared to other divisions? Again, we will use barplot(s) to present the visualization. (12.5%)\n(you can use either geom_bar() or geom_col())\n\ndivision_stack&lt;- ggplot(aus_long, aes(fill = Response, x = Division, y = Count)) + \n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(title = \"The National Distribution of Resesponse\", x = NULL, y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 100)) +\n  theme(axis.text.x=element_text(angle=60, hjust=1))\n\ndivision_stack\n\n\n\n\n\n\n\n\nMaybe it is difficult to see in stacked bars. Let’s plot regular side-by-side bar plots for each devision and use facet to combine multiple plots.\n\ndivision_facet&lt;-aus_long|&gt;\n  mutate(Response = as_factor(Response),\n         Response = fct_relevel(Response, \"Yes\", \"No\", \"Illegible\"),\n         Division=str_remove(Division,\" Divisions\")) |&gt;\n  group_by(Division,Response)|&gt;\n  summarise(Count = sum(Count))|&gt;\n  group_by(Division)|&gt;\n  mutate(perc = Count/sum(Count))|&gt;\n  ggplot(aes(y=perc, x=Response,fill=Response))+\n  geom_col()+\n  facet_wrap(vars(Division))+\n  labs(title = \"The Distribution of Resesponse by Division\") +\n  scale_y_continuous(name= \"Percent of Citizens\", \n                     label = scales::percent)+\n  theme(axis.text.x=element_text(angle = 60, hjust=1))\n\ndivision_facet"
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#part-3.-practice-plotting-with-a-dataset-of-your-choice-25-of-the-total-grade",
    "href": "Project2/challenge_4_Fall23_solutions.html#part-3.-practice-plotting-with-a-dataset-of-your-choice-25-of-the-total-grade",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Part 3. Practice plotting with a dataset of your choice (25% of the total grade)",
    "text": "Part 3. Practice plotting with a dataset of your choice (25% of the total grade)\nIn this part, you will choose data of your interests for graphing and plotting. This data can be tidy/ready-to-be-used or raw data that needs cleaning. If the data is very large (for example, more than 20 columns), you should definitely subset the data by selecting less than 10 variables of your interests to avoid taking too much room in your R memory.\n\nInclude a link to the data page (this page should include the introduction or description and the link to download this dataset). (2%)\nRead the data you choose and briefly answer the following questions. (Optional: you may need to subset, clean, and transform the data if necessary). (8%)\n\n#type of your code/command here.\n\n(1) what is the structure (dimension) of the data;\n(2) what is the unit of observation?\n(3) what does each column mean in this data?\nChoose two columns/variables of your interests. Plot one univariate graph for each of the variables. (5%)\n\n#type of your code/command here.\n\n\n\n\nChoose a pair of variables you suspect or hypothesize may be correlated and a graph (scatter plot or barplot) using them. Based on the visual evidence, do you see any potential correlation between the two variables (10%)\n\n#type of your code/command here."
  },
  {
    "objectID": "Project2/challenge_4_Fall23_solutions.html#appendix-sources-for-data-to-be-used-in-part-3",
    "href": "Project2/challenge_4_Fall23_solutions.html#appendix-sources-for-data-to-be-used-in-part-3",
    "title": "Challenge_4: Intro to Visulization: Univariate and Multivariate Graphs",
    "section": "Appendix: sources for data to be used in Part 3",
    "text": "Appendix: sources for data to be used in Part 3\nHere are some online sources and popular Online Dataset Hub:\n\nMany US governments (usually at the federal and state levels),  bureaus, and departments have open data archives on their websites, allowing the public to access, download, and use them. Just use Google to search for them.\n\n\n\nThe Harvard Dataverse Repository is a free data repository open to all researchers from any discipline, inside and outside the Harvard community, where you can share, archive, cite, access, and explore research data. Each individual Dataverse collection is a customizable collection of datasets (or a virtual repository) for organizing, managing, and showcasing datasets.\n\n\n\nInter-university Consortium for Political and Social Research (ICPSR) of the University of Michigan-Ann Arbor provides leadership and training in data access, curation, and methods of analysis for the social science research community. \n\n\n\nUN: https://data.un.org/\n\n\n\nOECD Data:  economic and development data of the most developed countries in the world.\n\n\n\nThe upper five sources are mainly for social science data; there is another very big community and open data archives for machine-learning and data science: Kaggle."
  },
  {
    "objectID": "storyboard.html",
    "href": "storyboard.html",
    "title": "Storyboard Commentary",
    "section": "",
    "text": "Frame 1\n\nSome commentary about Frame 1.\n\n\nFrame 2\n\nSome commentary about Frame 2."
  }
]